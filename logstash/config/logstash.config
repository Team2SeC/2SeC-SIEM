#==============================================================================
# 2SeC SIEM Logstash Pipeline Configuration  
# Kinesis Data Stream → Processing → OpenSearch
#==============================================================================

input {
  kinesis {
    kinesis_stream_name => "${KINESIS_STREAM_NAME}"
    region => "${AWS_REGION}"
    application_name => "${PROJECT_NAME}-logstash-consumer"  # ✅ 환경변수 사용
    initial_position => "TRIM_HORIZON"
    checkpoint_interval => 10
    metrics => "cloudwatch"
    
    codec => json {
      charset => "UTF-8"
    }
    
    add_tag => ["kinesis", "${PROJECT_NAME}", "raw"]  # ✅ 환경변수 사용
  }
}

filter {
  #---------------------------------------------------------------------------
  # 1. 기본 필드 정리
  #---------------------------------------------------------------------------
  mutate {
    remove_field => ["@version"]
    add_field => {
      "pipeline" => "${PROJECT_NAME}-logstash"           # ✅ 환경변수 사용
      "processed_at" => "%{+yyyy.MM.dd HH:mm:ss}"
      "data_source" => "${PROJECT_NAME}-siem"            # ✅ 환경변수 사용
      "index_date" => "%{+yyyy.MM.dd}"
    }
  }
  
  # 타임스탬프 정규화
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss", "yyyy/MM/dd HH:mm:ss" ]
      target => "@timestamp"
    }
  }
  
  #---------------------------------------------------------------------------
  # 2. 로그 타입별 파싱
  #---------------------------------------------------------------------------
  if [log_type] == "web" or "web" in [tags] {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
      add_tag => ["web_parsed"]
    }
    
    if [agent] {
      useragent {
        source => "agent"
        target => "user_agent"
      }
    }
    
    if [clientip] {
      mutate {
        add_field => { "source_ip" => "%{clientip}" }
      }
    }
  }
  else if [log_type] == "system" or "system" in [tags] {
    grok {
      match => { "message" => "%{SYSLOGLINE}" }
      add_tag => ["system_parsed"]
    }
  }
  else if [log_type] == "security" or "security" in [tags] {
    mutate {
      add_tag => ["security_event"]
    }
    
    # 위험도 레벨 설정
    if [event_type] in ["sql_injection", "xss"] {
      mutate {
        add_field => { "risk_level" => "high" }
      }
    } else if [event_type] == "brute_force" {
      mutate {
        add_field => { "risk_level" => "medium" }
      }
    } else {
      mutate {
        add_field => { "risk_level" => "low" }
      }
    }
  }
  
  #---------------------------------------------------------------------------
  # 3. 최종 정제
  #---------------------------------------------------------------------------
  mutate {
    remove_field => ["host", "path", "[@metadata]"]
  }
  
  # 파싱 상태 태깅
  if "_grokparsefailure" in [tags] {
    mutate {
      add_tag => ["parse_error"]
      add_field => { "parse_status" => "failed" }
    }
  } else {
    mutate {
      add_field => { "parse_status" => "success" }
    }
  }
}

#==============================================================================
# OUTPUT
#==============================================================================
output {
  # OpenSearch로 메인 데이터 전송
  opensearch {
    hosts => ["https://${OPENSEARCH_HOST}"]
    index => "${PROJECT_NAME}-siem-%{+yyyy.MM.dd}"     # ✅ 환경변수 사용
    
    # 인증
    user => "${OPENSEARCH_USERNAME}"                    # ✅ 환경변수 사용
    password => "${OPENSEARCH_PASSWORD}"                # ✅ Secrets Manager
    
    # SSL 설정
    ssl => true
    ssl_certificate_verification => false
    
    # 에러 처리
    retry_on_conflict => 3
  }
  
  # 에러 로그만 CloudWatch로 백업
  if "parse_error" in [tags] {
    cloudwatch_logs {
      log_group_name => "/aws/logstash/${PROJECT_NAME}-errors"  # ✅ 환경변수 사용
      log_stream_name => "parse-errors-%{+yyyy-MM-dd}"
      region => "${AWS_REGION}"
    }
  }
}